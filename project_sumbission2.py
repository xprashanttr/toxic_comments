# -*- coding: utf-8 -*-
"""Project_Sumbission2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eIC0cmnNItsOZyKl8CeTgtZf9j1bqhFt

##**Data download**

Data setup section has steps to copy data from Google Drive to Google-Colab.
First submission had steps to download data from Kaggle directly. However, I faced Kaggle API authentication issues this time. It was strange but couldnt help.

---

Original data is Available at kaggle website : https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data


Both test.csv.zip and train.csv.zip should be kep at home directory of google drive before executing this code.

---

If this has to be avoided, then manual upload of files are required on Google-colab using below code :


*from google.colab import files*

*files.upload()*
"""

'''
This will ask for authentication and access to google drive.
train.csv.zip & test.csv.zip &  should be placed in home folder of Google drive
'''
from googleapiclient.discovery import build
import io, os
from googleapiclient.http import MediaIoBaseDownload
from google.colab import auth
auth.authenticate_user()
drive_service = build('drive', 'v3')

#File Download

!rm -f *.zip *.csv

#!kaggle competitions download -c jigsaw-toxic-comment-classification-challenge
#train.csv download
results = drive_service.files().list(q="name = 'train.csv.zip'", fields="files(id)").execute()
kaggle_api_key = results.get('files', [])
filename = "/content/train.csv.zip"
os.makedirs(os.path.dirname(filename), exist_ok=True)
request = drive_service.files().get_media(fileId=kaggle_api_key[0]['id'])
fh = io.FileIO(filename, 'wb')
downloader = MediaIoBaseDownload(fh, request)
done = False
while done is False:
    status, done = downloader.next_chunk()
    print("train.csv.zip - Download %d%%." % int(status.progress() * 100))
os.chmod(filename, 600)

#test.csv download
results = drive_service.files().list(q="name = 'test.csv.zip'", fields="files(id)").execute()
kaggle_api_key = results.get('files', [])
filename = "/content/test.csv.zip"
os.makedirs(os.path.dirname(filename), exist_ok=True)
request = drive_service.files().get_media(fileId=kaggle_api_key[0]['id'])
fh = io.FileIO(filename, 'wb')
downloader = MediaIoBaseDownload(fh, request)
done = False
while done is False:
    status, done = downloader.next_chunk()
    print("test.csv.zip - Download %d%%." % int(status.progress() * 100))
os.chmod(filename, 600)


!unzip train.csv.zip

!unzip test.csv.zip

!ls -ltr

!wc -l *.csv

"""##**imports**"""

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import scipy.sparse as sp
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
import sklearn.metrics as metrics
from sklearn.multioutput import MultiOutputClassifier
import time
from sklearn.pipeline import Pipeline
from sklearn.multiclass import OneVsRestClassifier
from sklearn.linear_model import LogisticRegression
import seaborn as sns

"""##**Utilities**"""

def read_data(filename):
    return pd.read_csv(filename) 

def get_tfidf_vectorizer(sentences):
    x = TfidfVectorizer(max_df=0.5, max_features = 5000, min_df=2, stop_words='english',use_idf=True)
    x.fit(sentences)
    return x

def get_nrange_tfidf_vectorizer(sentences):
    x = TfidfVectorizer(max_df=0.5, ngram_range=(2,3), max_features = 5000,min_df=1, stop_words='english',use_idf=True)
    x.fit(sentences)
    return x

def get_vectors(model1, model2, sentences):
    x1 = model1.transform(sentences)
    x2 = model2.transform(sentences)
    return sp.hstack([x1, x2])

class ExtendedMultiOutputClassifier(MultiOutputClassifier):
    def transform(self, x):
        """ Add a transform method to the RF classifier to provide fit and transform methods """
        return np.concatenate(self.predict_proba(x), axis=1)

"""##**Train data read**"""

'''
Read data
'''

train_data = 'train.csv'

# utility definitions for easier handling of the dataset column names
TEXT_COLUMN = 'comment_text'
CLASS_TOXIC, CLASS_SEVER_TOXIC, CLASS_OBSCENE, CLASS_THREAT, CLASS_INSULT, CLASS_IDENTITY_HATE = ["toxic", "severe_toxic", "obscene", "threat","insult", "identity_hate"]
CLASSES = [CLASS_TOXIC, CLASS_SEVER_TOXIC, CLASS_OBSCENE, CLASS_THREAT, CLASS_INSULT, CLASS_IDENTITY_HATE]

# read the comments and associated classification data 
train_data_full_df = read_data(train_data)
print(train_data_full_df.shape)

"""##**Some basic data charecteristics of training data provided**"""

'''
Histogram of number of classes per datapoint
'''
labels, counts = np.unique(train_data_full_df[CLASSES].sum(axis=1), return_counts=True)
plt.bar(labels, counts, align='center')
plt.gca().set_title('Histogram of number of classes per comment')
plt.gca().set_xlabel('Number of classes per comment')
plt.gca().set_xticks(labels)
plt.show()

'''
Number of records per class type
'''
df_toxic = train_data_full_df.drop(['id', 'comment_text'], axis=1)
counts = []
categories = list(df_toxic.columns.values)
for i in categories: counts.append((i, df_toxic[i].sum()))
df_stats = pd.DataFrame(counts, columns=['category', 'number_of_comments'])
df_stats

df_stats.plot(x='category', y='number_of_comments', kind='bar', legend=False, grid=True, figsize=(8, 5))
plt.title('Number of comments per category')
plt.ylabel('Number of Occurrences', fontsize=12)
plt.xlabel('category', fontsize=12)

print('Percentage of comments that are not labelled:')
print(len(train_data_full_df[(train_data_full_df['toxic']==0) & (train_data_full_df['severe_toxic']==0) & (train_data_full_df['obscene']==0) & (train_data_full_df['threat']== 0) & (train_data_full_df['insult']==0) & (train_data_full_df['identity_hate']==0)]) / len(train_data_full_df))

'''
#words in comment texts
'''

comment_length = train_data_full_df.comment_text.str.len()
comment_length.hist(bins = np.arange(0,5000,50))

'''
Most of the comment text length are within 500 characters
Some outliers up to 4,000+ characters long.
'''

comment_length

comment_length.mean()

'''
missing comment in comment text column.
'''

print('Number of missing comments in training data:')
train_data_full_df['comment_text'].isnull().sum()

"""##**Data Preparation**"""

## shuffle and split the dataset stratified by the number of classifications of a data point
## for balancing across resulting modeling and evaluation datasets

#Parameters for StratifiedShuffleSplit
v_n_splits=10
v_train_size=0.8
v_test_size=0.2


print('\n Training and evaluation data building starts')
train_start_time = time.time()
sss = StratifiedShuffleSplit(n_splits=v_n_splits, train_size=v_train_size, test_size=v_test_size, random_state=0)
for train_index, test_index in sss.split(np.zeros(len(train_data_full_df)), train_data_full_df[CLASSES].sum(axis=1)): pass


train_data_modeling_df = train_data_full_df.iloc[train_index]
train_data_eval_df = train_data_full_df.iloc[test_index]


print('\n Training and evaluation data building ends - Time(Seconds) - %s'% int(time.time() - train_start_time))

"""##**Some basic data charecteristics of training and evaluation data prepared**"""

print('---------------- Training Data ----------------')
print('Number of records in Training data set: %d' %len(train_data_modeling_df))
for x in CLASSES: print('Number of records of type %s: %d' %(x, len(train_data_modeling_df[train_data_modeling_df[x]==1])))
#print('--------------------------------------------------------------')
print('---------------- Evaluation Data ----------------')
print('Number of records in Evaluation data set: %d' %len(train_data_eval_df))
for x in CLASSES: print('Number of records of type %s: %d' %(x, len(train_data_eval_df[train_data_eval_df[x]==1])))

'''
Histogram of number of classes per datapoint for training and evaluation dataset
'''
labels, counts = np.unique(train_data_modeling_df[CLASSES].sum(axis=1), return_counts=True)
plt.bar(labels, counts, align='center')
plt.gca().set_title('Histogram of number of classes per datapoint - Training dataset')
plt.gca().set_xlabel('Number of classes per datapoint')
plt.gca().set_xticks(labels)
plt.show()

labels, counts = np.unique(train_data_eval_df[CLASSES].sum(axis=1), return_counts=True)
plt.bar(labels, counts, align='center')
plt.gca().set_title('Histogram of number of classes per datapoint - Evaluation dataset')
plt.gca().set_xlabel('Number of classes per datapoint')
plt.gca().set_xticks(labels)
plt.show()

'''
#words in comment texts
'''

comment_length_moodel = train_data_modeling_df.comment_text.str.len()
comment_length_moodel.hist(bins = np.arange(0,5000,50))

'''
Most of the comment text length are within 500 characters
Some outliers up to 4,000+ characters long.


'''

'''
#words in comment texts
'''

comment_length_moodel = train_data_eval_df.comment_text.str.len()
comment_length_moodel.hist(bins = np.arange(0,5000,50))

'''
Most of the comment text length are within 500 characters
Some outliers up to 4,000+ characters long.


'''

"""##**Embedding**"""

## doc2vec model from model training data
print('\n Building doc2vec model starts')
doc2vec_model1 = get_tfidf_vectorizer(train_data_modeling_df[TEXT_COLUMN])
doc2vec_model2 = get_nrange_tfidf_vectorizer(train_data_modeling_df[TEXT_COLUMN])
print('\n Building doc2vec model Ends')

## prepare the data vectors
print('\n Bullding vectors starts')
X = get_vectors(doc2vec_model1, doc2vec_model2, train_data_modeling_df[TEXT_COLUMN])
print('\n Bullding vectors ends')

"""## Benchmark model"""

## LR
print('\n Building LR starts')

lr_start_time = time.time()


LogReg_pipeline = Pipeline([
                ('tfidf', TfidfVectorizer(max_features = 5000, stop_words='english',use_idf=True)),
                ('clf', OneVsRestClassifier(LogisticRegression(), n_jobs=-1)),
            ])

aucp=[]

for category in CLASSES:
    #print('Training {}'.format(category))
    print('\tTraining %s'%category)
    # train
    LogReg_pipeline.fit(train_data_modeling_df.comment_text, train_data_modeling_df[category])
    # predict
    prediction = LogReg_pipeline.predict(train_data_eval_df.comment_text)
    # Evaluate
    aucp.append(metrics.roc_auc_score(train_data_eval_df[category], prediction))
    #break


print('\t\tMean AUC Benchmark : %f' %np.mean(aucp))

print('\n Building LR ends - Time(Seconds) - %s'% int(time.time() - lr_start_time))

"""##**Project submission Model**"""

'''

Eval for pipeline model

'''

print('\t Creating pipeline model starts')

pm_start_time = time.time()

piperf = ExtendedMultiOutputClassifier(RandomForestClassifier(n_estimators = 100, n_jobs=-1))
pipemlp = MLPClassifier(solver='sgd', activation='logistic', learning_rate='adaptive', hidden_layer_sizes=(100, 100), random_state=1, momentum=0.9, alpha=1e-6)

pipm = Pipeline([('rf', piperf), ('nn', pipemlp)]) #--Pipeline 
pipm.fit(X, train_data_modeling_df[CLASSES])

print('\n Creating pipeline model starts ends - Time(Seconds) - %s'% int(time.time() - pm_start_time))

print('\t Predicting classes pipeline starts')
pred_start_time = time.time()
predicted_pipe = pipm.predict_proba(X_test)

predicted_pipeDf = pd.DataFrame(predicted_pipe)
predicted_pipeDf.columns = CLASSES

print('\n Predicting classes pipeline starts - Time(Seconds) - %s'% int(time.time() - pred_start_time))


print('\t Evaluating model ROC AUC pipeline\n')

# mean auc
aucs_pipeline = list(map(lambda klass: metrics.roc_auc_score(train_data_eval_df[klass], predicted_pipeDf[klass]), CLASSES))

print('\t\t-----------------------')
print('\t\t splits: %f' %v_n_splits)
print('\t\t train_size: %f' %v_train_size)
print('\t\t test_size: %f' %v_test_size)

print('\t\t Mean AUC pipeline: %f' %np.mean(aucs_pipeline))

print('\t\t-----------------------')

print('\t Evaluating model ROC AUC pipeline ends\n')

X_test = get_vectors(doc2vec_model1, doc2vec_model2, train_data_eval_df[TEXT_COLUMN])


print('\t Predicting classes pipeline starts')
pred_start_time = time.time()
predicted_pipe = pipm.predict_proba(X_test)

predicted_pipeDf = pd.DataFrame(predicted_pipe)
predicted_pipeDf.columns = CLASSES

print('\n Predicting classes pipeline starts - Time(Seconds) - %s'% int(time.time() - pred_start_time))


print('\t Evaluating model ROC AUC pipeline\n')

# mean auc
aucs_pipeline = list(map(lambda klass: metrics.roc_auc_score(train_data_eval_df[klass], predicted_pipeDf[klass]), CLASSES))

print('\t\t-----------------------')
print('\t\t splits: %f' %v_n_splits)
print('\t\t train_size: %f' %v_train_size)
print('\t\t test_size: %f' %v_test_size)

print('\t\t Mean AUC pipeline: %f' %np.mean(aucs_pipeline))

print('\t\t-----------------------')

print('\t Evaluating model ROC AUC pipeline ends\n')

"""##**Prediction and submission**"""

#Remove existing submission files
!rm -f submission.*
!ls -ltr submission.*

'''
Test Data
'''
test_data = 'test.csv'
test_df = pd.read_csv(test_data)
test_df.info()

'''
With Pipeline
'''
print('\n Pipeline Prediction starts')
predict_start_time = time.time()

predX_sub = get_vectors(doc2vec_model1, doc2vec_model2, test_df[TEXT_COLUMN])
predy_sub = pipm.predict_proba(predX_sub)
predy_subdf = pd.DataFrame(data=predy_sub)


predy_subdf.columns = CLASSES
pipesubmission_file = 'submission_pipe.csv'
submit_df = pd.concat([test_df['id'], predy_subdf], axis=1)
submit_df.head()
submit_df.to_csv(pipesubmission_file, index=False)


print('\n Pipeline Prediction ends - Time(Seconds) - %s'% int(time.time() - predict_start_time))

#Count of records in submission file Should be 153165
!wc -l submission*.csv
#Zip the file
#!zip submission.zip submission.csv
!zip submission_pipe.zip submission_pipe.csv

#Download submission file
from google.colab import files
#files.download('submission.zip')
files.download('submission_pipe.zip')